{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.legacy_seq2seq as seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN_CHAR = '['\n",
    "END_CHAR = ']'\n",
    "UNKNOWN_CHAR = '*'\n",
    "# MAX_LENGTH = 100\n",
    "MAX_LENGTH = 40\n",
    "# MIN_LENGTH = 10\n",
    "MIN_LENGTH = 8\n",
    "max_words = 3000\n",
    "# epochs = 1\n",
    "epochs = 20\n",
    "# poetry_file = './data/poetry.txt'\n",
    "# 改成对联数据所在的目录，相对路径和绝对路径均可\n",
    "poetry_file = './data/couples.txt'\n",
    "\n",
    "save_dir = 'log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 唐诗数据封装类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.poetry_file = poetry_file\n",
    "        self.load()\n",
    "        self.create_batches()\n",
    "\n",
    "    def load(self):\n",
    "        def handle(line):\n",
    "            \"\"\"\n",
    "            处理过长的唐诗，并为每首诗添加开始符 '[' 和结束符 ']'\n",
    "            \"\"\"\n",
    "            if len(line) > MAX_LENGTH:\n",
    "                index_end = line.rfind('。', 0, MAX_LENGTH)\n",
    "                index_end = index_end if index_end > 0 else MAX_LENGTH\n",
    "                line = line[:index_end + 1]\n",
    "            return BEGIN_CHAR + line + END_CHAR\n",
    "\n",
    "        self.poetrys = []\n",
    "        lines = open(self.poetry_file, encoding='utf-8')\n",
    "        # self.poetrys = [line.strip().replace(' ', '').split(':')[1] for line in lines]\n",
    "        # 对联数据中没有标题，且不存在分号 ： ，故改成下面的代码\n",
    "        self.poetrys = [line.strip().replace(' ', '') for line in lines]\n",
    "        \n",
    "        # 过滤短的唐诗\n",
    "        self.poetrys = [handle(line) for line in self.poetrys if len(line) > MIN_LENGTH]\n",
    "        \n",
    "        # 统计所有出现的字\n",
    "        words = []\n",
    "        for poetry in self.poetrys:\n",
    "            words += [word for word in poetry]\n",
    "        counter = collections.Counter(words)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        words, _ = zip(*count_pairs)\n",
    "        \n",
    "        # 取出现频率最高的词的数量组成字典，不在字典中的字用'*'代替\n",
    "        words_size = min(max_words, len(words))\n",
    "        self.words = words[:words_size] + (UNKNOWN_CHAR,)\n",
    "        self.words_size = len(self.words)\n",
    "\n",
    "        # 建立字典char2id 和 id2char\n",
    "        self.char2id_dict = {w: i for i, w in enumerate(self.words)}\n",
    "        self.id2char_dict = {i: w for i, w in enumerate(self.words)}\n",
    "        self.unknow_char = self.char2id_dict.get(UNKNOWN_CHAR)\n",
    "        self.char2id = lambda char: self.char2id_dict.get(char, self.unknow_char)\n",
    "        self.id2char = lambda num: self.id2char_dict.get(num)\n",
    "        \n",
    "        # 诗句->向量\n",
    "        self.poetrys = sorted(self.poetrys, key=lambda line: len(line))\n",
    "        self.poetrys_vector = [list(map(self.char2id, poetry)) for poetry in self.poetrys]\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.n_size = len(self.poetrys_vector) // self.batch_size\n",
    "        self.poetrys_vector = self.poetrys_vector[:self.n_size * self.batch_size]\n",
    "        self.x_batches = []\n",
    "        self.y_batches = []\n",
    "        for i in range(self.n_size):\n",
    "            batches = self.poetrys_vector[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            length = max(map(len, batches))\n",
    "            \n",
    "            # 把每首诗填充成长度为 length (每个batch中最长的诗) 的向量\n",
    "            for row in range(self.batch_size):\n",
    "                if len(batches[row]) < length:\n",
    "                    r = length - len(batches[row])\n",
    "                    batches[row][len(batches[row]): length] = [self.unknow_char] * r\n",
    "            # 网络的输入是xdata,输出是ydata。ydata由xdata向后移动一位得到        \n",
    "            xdata = np.array(batches)\n",
    "            ydata = np.copy(xdata)\n",
    "            ydata[:, :-1] = xdata[:, 1:]\n",
    "            self.x_batches.append(xdata)\n",
    "            self.y_batches.append(ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, data, model='lstm', infer=False):\n",
    "        # 设置RNN网络层超参数\n",
    "        self.rnn_size = 128\n",
    "        self.n_layers = 2\n",
    "        \n",
    "        # 如果是测试模式，设置batch_size为1，即每次只预测一首诗\n",
    "        if infer:\n",
    "            self.batch_size = 1\n",
    "        else:\n",
    "            self.batch_size = data.batch_size\n",
    "        \n",
    "        # 可选的RNN变体，本次实验选LSTM\n",
    "        if model == 'rnn':\n",
    "            cell_rnn = tf.nn.rnn_cell.BasicRNNCell\n",
    "        elif model == 'gru':\n",
    "            cell_rnn = tf.nn.rnn_cell.GRUCell\n",
    "        elif model == 'lstm':\n",
    "            cell_rnn = tf.nn.rnn_cell.LSTMCell\n",
    "       \n",
    "        # 开始构建图\n",
    "        \n",
    "        # 2层LSTM堆叠作为诗的表示层\n",
    "        cell = cell_rnn(self.rnn_size, name='basic_lstm_cell',)\n",
    "        self.cell = tf.nn.rnn_cell.MultiRNNCell([cell] * self.n_layers)\n",
    "        \n",
    "        # 设置计算图xdata和ydata的占位符，可理解为形参，在执行run方法将实参传入\n",
    "        self.x_tf = tf.placeholder(tf.int32, [self.batch_size, None])\n",
    "        self.y_tf = tf.placeholder(tf.int32, [self.batch_size, None])\n",
    "        \n",
    "        #LSTM网络层初始化，每次初始化一个batch\n",
    "        self.initial_state = self.cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        # 设置变量空间,可设置多个参数，这里只是简单的使用默认设置。（类似于C++命名空间）\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [self.rnn_size, data.words_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [data.words_size])\n",
    "            # 将one-hot向量映射成符合RNN网络输入的向量\n",
    "            embedding = tf.get_variable(\"embedding\", [data.words_size, self.rnn_size])\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.x_tf)\n",
    "\n",
    "        # 前馈计算\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(self.cell, \n",
    "                                                 inputs, \n",
    "                                                 initial_state=self.initial_state, \n",
    "                                                 scope='rnnlm') \n",
    "        \"\"\"\n",
    "        创建批数据的时候，每首诗填充成长度为 length (每个batch中最长的诗),因此每个batch之间的shape可能不一致。\n",
    "        tf.nn.dynamic_rnn适合输入的shape不同的情况，tf.nn.rnn必须要求输入的shape必须一致。\n",
    "        上面的initial_state，final_state神经网络隐藏层单元状态值\n",
    "        \"\"\" \n",
    "        \n",
    "        self.output = tf.reshape(outputs, [-1, self.rnn_size])\n",
    "        self.logits = tf.matmul(self.output, softmax_w) + softmax_b\n",
    "        \n",
    "        # 字典所有词的概率分布，在测试时会用到\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        self.final_state = final_state\n",
    "        pred = tf.reshape(self.y_tf, [-1])\n",
    "        # seq2seq\n",
    "        loss = seq2seq.sequence_loss_by_example([self.logits],\n",
    "                                                [pred],\n",
    "                                                [tf.ones_like(pred, dtype=tf.float32)],)\n",
    "        self.cost = tf.reduce_mean(loss)\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \"\"\"\n",
    "        gradient clipping(梯度修剪)的引入是为了处理梯度消失或者梯度爆炸的问题。\n",
    "        当在一次迭代中权重的更新过于迅猛的话，很容易导致loss发散。\n",
    "        clipping让权重的更新限制在一个合适的范围。\n",
    "        apply_gradients：计算得到的梯度来更新对应的variable\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model):\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        elif len(os.listdir(save_dir))>0:\n",
    "            model_file = tf.train.latest_checkpoint(save_dir)\n",
    "            saver.restore(sess, model_file)\n",
    "        n = 0\n",
    "        for epoch in range(epochs):\n",
    "            # 逐级降低学习率\n",
    "            sess.run(tf.assign(model.learning_rate, 0.002 * (0.97 ** epoch)))\n",
    "            pointer = 0\n",
    "            for batch in range(data.n_size):\n",
    "                n += 1\n",
    "                feed_dict = {model.x_tf: data.x_batches[pointer], model.y_tf: data.y_batches[pointer]}\n",
    "                pointer += 1\n",
    "                train_loss, _, _ = sess.run([model.cost, model.final_state, model.train_op], feed_dict=feed_dict)\n",
    "                info = \"{}/{} (epoch {}) | train_loss {:.3f}\" \\\n",
    "                    .format(epoch * data.n_size + batch,\n",
    "                            epochs * data.n_size, epoch, train_loss)\n",
    "                print(info)\n",
    "                # save\n",
    "                if (epoch * data.n_size + batch) % 1000 == 0 \\\n",
    "                        or (epoch == epochs-1 and batch == data.n_size-1):\n",
    "                    checkpoint_path = os.path.join(save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path, global_step=n)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, model, head=u''):\n",
    "    def to_word(weights):\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        sa = int(np.searchsorted(t, np.random.rand(1) * s))\n",
    "        return data.id2char(sa)\n",
    "\n",
    "    for word in head:\n",
    "        if word not in data.words:\n",
    "            return u'{} 不在字典中'.format(word)\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        model_file = tf.train.latest_checkpoint(save_dir)\n",
    "        # print(model_file)\n",
    "        saver.restore(sess, model_file)\n",
    "\n",
    "        if head:\n",
    "            print('生成藏头诗 ---> ', head)\n",
    "            poem = BEGIN_CHAR\n",
    "            for head_word in head:\n",
    "                poem += head_word\n",
    "                x = np.array([list(map(data.char2id, poem))])\n",
    "                state = sess.run(model.cell.zero_state(1, tf.float32))\n",
    "                feed_dict = {model.x_tf: x, model.initial_state: state}\n",
    "                [probs, state] = sess.run([model.probs, model.final_state], feed_dict)\n",
    "                word = to_word(probs[-1])\n",
    "                while word != u'，' and word != u'。':\n",
    "                    poem += word\n",
    "                    x = np.zeros((1, 1))\n",
    "                    x[0, 0] = data.char2id(word)\n",
    "                    [probs, state] = sess.run([model.probs, model.final_state],\n",
    "                                              {model.x_tf: x, model.initial_state: state})\n",
    "                    word = to_word(probs[-1])\n",
    "                poem += word\n",
    "            return poem[1:]\n",
    "        else:\n",
    "            poem = ''\n",
    "            head = BEGIN_CHAR\n",
    "            x = np.array([list(map(data.char2id, head))])\n",
    "            state = sess.run(model.cell.zero_state(1, tf.float32))\n",
    "            feed_dict = {model.x_tf: x, model.initial_state: state}\n",
    "            [probs, state] = sess.run([model.probs, model.final_state], feed_dict)\n",
    "            word = to_word(probs[-1])\n",
    "            while word != END_CHAR:\n",
    "                poem += word\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = data.char2id(word)\n",
    "                [probs, state] = sess.run([model.probs, model.final_state],\n",
    "                                          {model.x_tf: x, model.initial_state: state})\n",
    "                word = to_word(probs[-1])\n",
    "            return poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # 设置用哪一块GPU\n",
    "mode = \"test\"  # 默认训练模式\n",
    "head = u'且欲'   #设置开头的字\n",
    "assert mode in [\"train\",\"test\"]\n",
    "print(\"%s mode\" % mode)\n",
    "print(\"head:%s\" % head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练测试入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'train':\n",
    "    data = Data()\n",
    "    model = Model(data=data, infer=False)\n",
    "    train(data, model) \n",
    "        \n",
    "elif mode == 'test':\n",
    "    assert(head!=u''and len(head)==2)\n",
    "    data = Data()\n",
    "    model = Model(data=data, infer=True)\n",
    "    poem = test(data, model, head=head)\n",
    "    print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
